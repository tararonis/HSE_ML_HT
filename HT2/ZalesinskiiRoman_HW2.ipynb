{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашня работа 2.\n",
    "### Логистическая регрессия. Работа с признаками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 15 баллов. Можно получить 4.5 бонусных балла.\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всем удачи!👒 Тут она точно пригодится.\n"
     ]
    }
   ],
   "source": [
    "print('Всем удачи!👒 Тут она точно пригодится.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T18:11:48.202066Z",
     "start_time": "2019-10-16T18:11:46.362572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 14:59:39.416248: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-23 14:59:39.661504: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-23 14:59:39.661582: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-23 14:59:39.705694: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-23 14:59:39.808070: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-23 14:59:39.809649: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-23 14:59:41.050427: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Логистическая регрессия своими руками (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T07:36:04.765536Z",
     "start_time": "2018-10-12T07:35:57.814973Z"
    }
   },
   "source": [
    "Реализуйте логистическую регрессию, обучаемую с помощью:\n",
    "\n",
    "**Задание 1 (1.5 балла). Градиентного спуска**\n",
    "\n",
    "**Задание 2 (1.5 балла). Стохастического градиентного спуска**\n",
    "\n",
    "Во всех пунктах необходимо соблюдать следующие условия:\n",
    "- Градиентный спуск необходимо записать в векторном виде\n",
    "- Циклы средствами python допускается использовать только для итераций градиентного спуска;\n",
    "- В качестве критерия останова необходимо использовать (одновременно):\n",
    "\n",
    "        проверку на евклидову норму разности весов на двух соседних итерациях (например, меньше некоторого малого числа порядка $10^{-6}$) задаваемого параметром `tolerance`;\n",
    "\n",
    "\n",
    "        достижение максимального числа итераций (например, 10000), задаваемого параметром `max_iter`.\n",
    "\n",
    "Чтобы проследить, что оптимизационный процесс действительно сходится, будем использовать атрибут класса loss_history. В нём после вызова метода fit должны содержаться значения функции потерь для всех итераций, начиная с первой (до совершения первого шага по антиградиенту);\n",
    "\n",
    "Инициализировать веса можно случайным образом или нулевым вектором."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полезно [почитать](https://scikit-learn.org/stable/developers/develop.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T18:11:50.932537Z",
     "start_time": "2019-10-16T18:11:50.752839Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LogReg(BaseEstimator):\n",
    "    def __init__(self, \n",
    "                gd_type='stochastic', \n",
    "                tolerance=1e-4,\n",
    "                max_iter=1000,\n",
    "                w0=None,\n",
    "                eta=1e-2,\n",
    "                batch_size=100):\n",
    "        \"\"\"\n",
    "        gd_type: 'full' or 'stochastic' \n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) — init weights\n",
    "        eta: learning rate\n",
    "        batch_size: number of training instances\n",
    "        \"\"\"\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        #self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "        self.batch_size=batch_size\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        self.loss_history = []\n",
    "        ell, d = X.shape\n",
    "        X = np.array(X)\n",
    "        y = y.reshape(ell, 1)\n",
    "        \n",
    "        if not self.w0:\n",
    "            self.w0 = np.random.rand(d, 1)\n",
    "        self.w = self.w0\n",
    "        \n",
    "        if self.gd_type == 'full':\n",
    "            loss_0 = self.calc_loss(X, y)\n",
    "            self.loss_history.append(loss_0)\n",
    "            \n",
    "            for step in range(self.max_iter):\n",
    "                grad = self.calc_gradient(X, y)\n",
    "                self.w -= self.eta * grad.reshape(d, 1)\n",
    "                \n",
    "                loss = self.calc_loss(X, y)\n",
    "                self.loss_history.append(loss)\n",
    "                \n",
    "                diff_stop = sum(abs(np.diff(self.loss_history[-3:])))\n",
    "                \n",
    "                if step > 3 and diff_stop <= self.tolerance:\n",
    "                    break\n",
    "        \n",
    "        elif self.gd_type == 'stochastic':\n",
    "            if self.batch_size > ell:\n",
    "                raise Exception(\n",
    "                    '''\n",
    "                    The batch size is bigger than the number of samples in the data.\n",
    "                    Please, use full gradient descent type insted (gd_type=\"full\").\n",
    "                    ''')\n",
    "            \n",
    "            for step in range(self.max_iter):\n",
    "                indices = np.random.randint(0, ell, size=self.batch_size)\n",
    "                X_batch = X[indices]\n",
    "                y_batch = y[indices]\n",
    "                \n",
    "                if step == 0:\n",
    "                    loss_0 = self.calc_loss(X_batch, y_batch)\n",
    "                    self.loss_history.append(loss_0)\n",
    "                \n",
    "                grad = self.calc_gradient(X_batch, y_batch)\n",
    "                reg = 1 / (step + 1)\n",
    "                self.w -= reg * self.eta * grad.reshape(d, 1)\n",
    "                \n",
    "                loss = self.calc_loss(X_batch, y_batch)\n",
    "                self.loss_history.append(loss)\n",
    "                \n",
    "                diff_stop = sum(abs(np.diff(self.loss_history[-3:])))\n",
    "                \n",
    "                if step > 3 and diff_stop <= self.tolerance:\n",
    "                    break\n",
    "        \n",
    "        return self\n",
    "       \n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        X = np.array(X)\n",
    "        args = X @ self.w\n",
    "        probas = 1 / (1 + np.exp(-args))\n",
    "\n",
    "        return probas\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        X = np.array(X)\n",
    "        pred_probas = self.predict_proba(X)\n",
    "        pred_classes = (pred_probas >= 0.5).astype(int)\n",
    "\n",
    "        return pred_classes\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d) (ell can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        X = np.array(X)\n",
    "        y = y.reshape(X.shape[0], 1)\n",
    "        pred = self.predict_proba(X)\n",
    "        grad = (pred - y).reshape(1, X.shape[0]) @ X\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\" \n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        X = np.array(X)\n",
    "        y = y.reshape(X.shape[0], 1)\n",
    "        pred = self.predict_proba(X)\n",
    "        loss = np.mean(-y * np.log(1e-4 + pred) - (1 - y) * np.log(1 + 1e-4 - pred))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Синтетические данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roman/.local/lib/python3.10/site-packages/IPython/core/magics/pylab.py:162: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=100000, n_features=20, n_informative=10, n_redundant=10,\n",
    "    random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важно:** далее предполагается, что вы используете собственную реализацию логистической регрессии.\n",
    "Если с написанием класса возникли проблемы, используйте реализацию sklearn, чтобы не терять баллы за остальные задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3 (1 балл)**\n",
    "\n",
    "Обучите логистическую регрессию на синтетических данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T20:39:43.088969Z",
     "start_time": "2018-10-11T20:39:43.084985Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1298403/4110918471.py:101: RuntimeWarning: overflow encountered in exp\n",
      "  probas = 1 / (1 + np.exp(-args))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.13 s, sys: 2.83 s, total: 3.95 s\n",
      "Wall time: 557 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogReg(batch_size=500, eta=0.2,\n",
       "       w0=array([[ 36.43491959],\n",
       "       [ 13.98873486],\n",
       "       [-49.47743717],\n",
       "       [-24.86641153],\n",
       "       [ 11.46544124],\n",
       "       [ -2.90869775],\n",
       "       [ 24.09968042],\n",
       "       [-15.81520738],\n",
       "       [ 37.16204038],\n",
       "       [  1.77040755],\n",
       "       [ -5.83261148],\n",
       "       [ 13.74160646],\n",
       "       [ 27.03900797],\n",
       "       [ -6.17428965],\n",
       "       [ -1.66565472],\n",
       "       [-42.60552012],\n",
       "       [  0.84414563],\n",
       "       [-27.75307758],\n",
       "       [ 30.30225918],\n",
       "       [  7.84548308]]))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogReg</label><div class=\"sk-toggleable__content\"><pre>LogReg(batch_size=500, eta=0.2,\n",
       "       w0=array([[ 36.43491959],\n",
       "       [ 13.98873486],\n",
       "       [-49.47743717],\n",
       "       [-24.86641153],\n",
       "       [ 11.46544124],\n",
       "       [ -2.90869775],\n",
       "       [ 24.09968042],\n",
       "       [-15.81520738],\n",
       "       [ 37.16204038],\n",
       "       [  1.77040755],\n",
       "       [ -5.83261148],\n",
       "       [ 13.74160646],\n",
       "       [ 27.03900797],\n",
       "       [ -6.17428965],\n",
       "       [ -1.66565472],\n",
       "       [-42.60552012],\n",
       "       [  0.84414563],\n",
       "       [-27.75307758],\n",
       "       [ 30.30225918],\n",
       "       [  7.84548308]]))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogReg(batch_size=500, eta=0.2,\n",
       "       w0=array([[ 36.43491959],\n",
       "       [ 13.98873486],\n",
       "       [-49.47743717],\n",
       "       [-24.86641153],\n",
       "       [ 11.46544124],\n",
       "       [ -2.90869775],\n",
       "       [ 24.09968042],\n",
       "       [-15.81520738],\n",
       "       [ 37.16204038],\n",
       "       [  1.77040755],\n",
       "       [ -5.83261148],\n",
       "       [ 13.74160646],\n",
       "       [ 27.03900797],\n",
       "       [ -6.17428965],\n",
       "       [ -1.66565472],\n",
       "       [-42.60552012],\n",
       "       [  0.84414563],\n",
       "       [-27.75307758],\n",
       "       [ 30.30225918],\n",
       "       [  7.84548308]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = LogReg(gd_type='stochastic', max_iter=1000, eta=0.2, batch_size=500)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На тестовой части посчитайте ROC-AUC, PR-AUC. Постройте ROC и PR кривые. Проинтерпретируйте результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, auc, roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1298403/4110918471.py:101: RuntimeWarning: overflow encountered in exp\n",
      "  probas = 1 / (1 + np.exp(-args))\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.847499950177458\n",
      "PR-AUC: 0.8852023136017241\n"
     ]
    }
   ],
   "source": [
    "# Рассчитываем ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"ROC-AUC:\", roc_auc)\n",
    "\n",
    "# Рассчитываем PR-AUC\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(\"PR-AUC:\", pr_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'True Positive Rate')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Строим ROC-кривую\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Precision')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Строим PR-кривую\n",
    "plt.figure()\n",
    "plt.plot(recall, precision)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Интерпретация результатов:\n",
    "- ROC-AUC является мерой качества классификации, которая оценивает способность модели различать между классами. Значение ROC-AUC равное 1 означает идеальную разделимость классов, а значение 0.5 означает случайное угадывание классов.\n",
    "В нашем случае значение ROC-AUC: 0.8471293122432306, что является хорошим, но не идеальным результатом.\n",
    "- PR-AUC также является мерой качества классификации, которая оценивает способность модели находить положительные примеры. \n",
    "PR-AUC: 0.8850783873743568 - достаточно хороший результат.\n",
    "- ROC-кривая показывает зависимость между True Positive Rate (чувствительность) и False Positive Rate (специфичность) при изменении порога классификации. Чем выше кривая поднимается, тем лучше. PR-кривая показывает зависимость между Precision (точность) и Recall (полнота) при изменении порога классификации. Чем ближе кривая к верхнему правому углу, тем лучше.\n",
    "Видим, что графики показывают результат достаточно хороший чтобы перестать подбираться параметры и перейти к следующим заданиям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Работа с категориальными переменными (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части мы научимся обрабатывать категориальные переменные, так как закодировать их в виде чисел недостаточно (это задаёт некоторый порядок, которого на категориальных переменных может и не быть). Существует два основных способа обработки категориальных значений:\n",
    "- One-hot-кодирование\n",
    "- Счётчики (CTR, mean-target кодирование, ...) — каждый категориальный признак заменяется на среднее значение целевой переменной по всем объектам, имеющим одинаковое значение в этом признаке.\n",
    "\n",
    "Начнём с one-hot-кодирования. Допустим наш категориальный признак $f_j(x)$ принимает значения из множества $C=\\{c_1, \\dots, c_m\\}$. Заменим его на $m$ бинарных признаков $b_1(x), \\dots, b_m(x)$, каждый из которых является индикатором одного из возможных категориальных значений:\n",
    "$$\n",
    "b_i(x) = [f_j(x) = c_i]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T20:41:54.913436Z",
     "start_time": "2018-10-11T20:41:54.907515Z"
    }
   },
   "source": [
    "__Подготовка данных.__\n",
    "\n",
    "Загрузим данные с конкурса  [Kaggle Porto Seguro’s Safe Driver Prediction](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction) (вам нужна только обучающая выборка). Задача состоит в определении водителей, которые в ближайший год воспользуются своей автомобильной страховкой (бинарная классификация). Но для нас важна будет не сама задача, а только её данные. При этом для задания мы немного модифицируем датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T07:36:04.765536Z",
     "start_time": "2018-10-12T07:35:57.814973Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv', index_col=0)\n",
    "target = data.target.values\n",
    "data = data.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  \\\n",
       "id                                                                      \n",
       "7           2              2          5              1              0   \n",
       "9           1              1          7              0              0   \n",
       "13          5              4          9              1              0   \n",
       "16          0              1          2              0              0   \n",
       "17          0              2          0              1              0   \n",
       "19          5              1          4              0              0   \n",
       "20          2              1          3              1              0   \n",
       "22          5              1          4              0              0   \n",
       "26          5              1          3              1              0   \n",
       "28          1              1          2              0              0   \n",
       "\n",
       "    ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  ps_ind_10_bin  \\\n",
       "id                                                                              \n",
       "7               0              1              0              0              0   \n",
       "9               0              0              1              0              0   \n",
       "13              0              0              1              0              0   \n",
       "16              1              0              0              0              0   \n",
       "17              1              0              0              0              0   \n",
       "19              0              0              0              1              0   \n",
       "20              0              1              0              0              0   \n",
       "22              1              0              0              0              0   \n",
       "26              0              0              1              0              0   \n",
       "28              0              1              0              0              0   \n",
       "\n",
       "    ...  ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  \\\n",
       "id  ...                                                                   \n",
       "7   ...           9           1           5           8               0   \n",
       "9   ...           3           1           1           9               0   \n",
       "13  ...           4           2           7           7               0   \n",
       "16  ...           2           2           4           9               0   \n",
       "17  ...           3           1           1           3               0   \n",
       "19  ...           4           2           0           9               0   \n",
       "20  ...           3           0           0          10               0   \n",
       "22  ...           7           1           3           6               1   \n",
       "26  ...           4           2           1           5               0   \n",
       "28  ...           3           5           0           6               0   \n",
       "\n",
       "    ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  \\\n",
       "id                                                                   \n",
       "7                1               1               0               0   \n",
       "9                1               1               0               1   \n",
       "13               1               1               0               1   \n",
       "16               0               0               0               0   \n",
       "17               0               0               1               1   \n",
       "19               1               0               1               1   \n",
       "20               1               0               0               1   \n",
       "22               0               1               0               1   \n",
       "26               1               0               0               0   \n",
       "28               1               0               0               1   \n",
       "\n",
       "    ps_calc_20_bin  \n",
       "id                  \n",
       "7                1  \n",
       "9                0  \n",
       "13               0  \n",
       "16               0  \n",
       "17               0  \n",
       "19               1  \n",
       "20               0  \n",
       "22               0  \n",
       "26               1  \n",
       "28               0  \n",
       "\n",
       "[10 rows x 57 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пересемплируем выборку так, чтобы положительных и отрицательных объектов в выборке было одинаковое число. Разделим на обучающую и тестовую выборки.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T07:36:05.368407Z",
     "start_time": "2018-10-12T07:36:04.770388Z"
    }
   },
   "outputs": [],
   "source": [
    "# some resampling\n",
    "np.random.seed(910)\n",
    "mask_plus = np.random.choice(np.where(target == 1)[0], 100000, replace=True)\n",
    "mask_zero = np.random.choice(np.where(target == 0)[0], 100000, replace=True)\n",
    "\n",
    "data = pd.concat((data.iloc[mask_plus], data.iloc[mask_zero]))\n",
    "target = np.hstack((target[mask_plus], target[mask_zero]))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1049183</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106872</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28943</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851826</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934813</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  \\\n",
       "id                                                                           \n",
       "1049183          0              1          5              0              0   \n",
       "106872           1              1          3              1              0   \n",
       "28943            6              2          7              1              0   \n",
       "851826           1              1          5              1              0   \n",
       "934813           0              1          1              0              4   \n",
       "\n",
       "         ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  \\\n",
       "id                                                                    \n",
       "1049183              1              0              0              0   \n",
       "106872               0              1              0              0   \n",
       "28943                0              0              1              0   \n",
       "851826               0              0              1              0   \n",
       "934813               1              0              0              0   \n",
       "\n",
       "         ps_ind_10_bin  ...  ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  \\\n",
       "id                      ...                                                   \n",
       "1049183              0  ...           4           3           4           6   \n",
       "106872               0  ...           9           0           4          12   \n",
       "28943                0  ...           4           1           2           7   \n",
       "851826               0  ...           6           2           2          10   \n",
       "934813               0  ...           9           1           2          14   \n",
       "\n",
       "         ps_calc_15_bin  ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  \\\n",
       "id                                                                        \n",
       "1049183               0               0               0               0   \n",
       "106872                0               1               1               0   \n",
       "28943                 0               1               1               0   \n",
       "851826                0               0               1               0   \n",
       "934813                0               1               1               0   \n",
       "\n",
       "         ps_calc_19_bin  ps_calc_20_bin  \n",
       "id                                       \n",
       "1049183               0               0  \n",
       "106872                0               0  \n",
       "28943                 1               0  \n",
       "851826                0               0  \n",
       "934813                0               0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 5 (0 баллов).** Посчитайте качество (в этом задании будем работать c ROC-AUC) на исходных признаках при применении логистической регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1298403/4110918471.py:101: RuntimeWarning: overflow encountered in exp\n",
      "  probas = 1 / (1 + np.exp(-args))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR-AUC: 0.7281708443777956\n",
      "CPU times: user 6.03 s, sys: 12.9 s, total: 19 s\n",
      "Wall time: 2.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_2 = LogReg(max_iter=1000, eta=0.2, batch_size=1500)\n",
    "model_2.fit(X_train, y_train)\n",
    "y_pred = model_2.predict(X_test)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(\"PR-AUC:\", pr_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 6 (0.5 балла).** Закодируйте все категориальные признаки с помощью one-hot-кодирования. Обучите логистическую регрессию и посмотрите, как изменилось качество модели (с тем, что было до кодирования). Измерьте время, потребовавшееся на обучение модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_columns = [column for column in X_train.columns if \"cat\" in column]\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "X_train_encoded = encoder.fit_transform(X_train[categorical_columns])\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out())\n",
    "X_train_encoded = pd.concat([X_train.drop(columns=categorical_columns), X_train_encoded], axis=1, join='inner')\n",
    "\n",
    "X_test_encoded = encoder.transform(X_test[categorical_columns])\n",
    "X_test_encoded = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out())\n",
    "X_test_encoded = pd.concat([X_test.drop(columns=categorical_columns), X_test_encoded], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = LogReg(batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1298403/4110918471.py:101: RuntimeWarning: overflow encountered in exp\n",
      "  probas = 1 / (1 + np.exp(-args))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 s, sys: 22.4 s, total: 34.1 s\n",
      "Wall time: 4.99 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogReg(batch_size=1000,\n",
       "       w0=array([[-0.05922669],\n",
       "       [-0.00886927],\n",
       "       [ 0.32656151],\n",
       "       [ 0.91701777],\n",
       "       [ 0.78672354],\n",
       "       [ 0.07132381],\n",
       "       [ 0.32016465],\n",
       "       [ 0.46559066],\n",
       "       [ 0.63447293],\n",
       "       [ 0.26140754],\n",
       "       [ 0.17513322],\n",
       "       [-0.04308835],\n",
       "       [ 0.24118993],\n",
       "       [ 0.77908788],\n",
       "       [ 0.2956205 ],\n",
       "       [ 0.54637369],\n",
       "       [ 0.67715105],\n",
       "       [ 0.2300086 ],\n",
       "       [-0.21572648],\n",
       "       [ 0.45203081],\n",
       "       [ 0.23398159],\n",
       "       [ 0.13809773],\n",
       "       [ 0.27090771],\n",
       "       [ 0.81683241],\n",
       "       [ 0....\n",
       "       [ 0.07277798],\n",
       "       [ 0.40024767],\n",
       "       [ 0.25483221],\n",
       "       [ 0.29498287],\n",
       "       [ 0.23015708],\n",
       "       [ 0.35364444],\n",
       "       [ 0.4470003 ],\n",
       "       [ 0.19220651],\n",
       "       [ 0.33440007],\n",
       "       [ 0.76201343],\n",
       "       [ 0.73911218],\n",
       "       [ 0.32547584],\n",
       "       [ 0.37819943],\n",
       "       [ 0.10667913],\n",
       "       [ 0.61625224],\n",
       "       [ 0.29957089],\n",
       "       [ 0.52686572],\n",
       "       [ 0.73844867],\n",
       "       [ 0.24180062],\n",
       "       [ 0.97689652],\n",
       "       [ 0.52715094],\n",
       "       [ 0.49581094],\n",
       "       [ 0.40625472],\n",
       "       [ 0.67021155],\n",
       "       [ 0.24211985],\n",
       "       [ 0.05364685],\n",
       "       [ 0.54554987]]))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogReg</label><div class=\"sk-toggleable__content\"><pre>LogReg(batch_size=1000,\n",
       "       w0=array([[-0.05922669],\n",
       "       [-0.00886927],\n",
       "       [ 0.32656151],\n",
       "       [ 0.91701777],\n",
       "       [ 0.78672354],\n",
       "       [ 0.07132381],\n",
       "       [ 0.32016465],\n",
       "       [ 0.46559066],\n",
       "       [ 0.63447293],\n",
       "       [ 0.26140754],\n",
       "       [ 0.17513322],\n",
       "       [-0.04308835],\n",
       "       [ 0.24118993],\n",
       "       [ 0.77908788],\n",
       "       [ 0.2956205 ],\n",
       "       [ 0.54637369],\n",
       "       [ 0.67715105],\n",
       "       [ 0.2300086 ],\n",
       "       [-0.21572648],\n",
       "       [ 0.45203081],\n",
       "       [ 0.23398159],\n",
       "       [ 0.13809773],\n",
       "       [ 0.27090771],\n",
       "       [ 0.81683241],\n",
       "       [ 0....\n",
       "       [ 0.07277798],\n",
       "       [ 0.40024767],\n",
       "       [ 0.25483221],\n",
       "       [ 0.29498287],\n",
       "       [ 0.23015708],\n",
       "       [ 0.35364444],\n",
       "       [ 0.4470003 ],\n",
       "       [ 0.19220651],\n",
       "       [ 0.33440007],\n",
       "       [ 0.76201343],\n",
       "       [ 0.73911218],\n",
       "       [ 0.32547584],\n",
       "       [ 0.37819943],\n",
       "       [ 0.10667913],\n",
       "       [ 0.61625224],\n",
       "       [ 0.29957089],\n",
       "       [ 0.52686572],\n",
       "       [ 0.73844867],\n",
       "       [ 0.24180062],\n",
       "       [ 0.97689652],\n",
       "       [ 0.52715094],\n",
       "       [ 0.49581094],\n",
       "       [ 0.40625472],\n",
       "       [ 0.67021155],\n",
       "       [ 0.24211985],\n",
       "       [ 0.05364685],\n",
       "       [ 0.54554987]]))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogReg(batch_size=1000,\n",
       "       w0=array([[-0.05922669],\n",
       "       [-0.00886927],\n",
       "       [ 0.32656151],\n",
       "       [ 0.91701777],\n",
       "       [ 0.78672354],\n",
       "       [ 0.07132381],\n",
       "       [ 0.32016465],\n",
       "       [ 0.46559066],\n",
       "       [ 0.63447293],\n",
       "       [ 0.26140754],\n",
       "       [ 0.17513322],\n",
       "       [-0.04308835],\n",
       "       [ 0.24118993],\n",
       "       [ 0.77908788],\n",
       "       [ 0.2956205 ],\n",
       "       [ 0.54637369],\n",
       "       [ 0.67715105],\n",
       "       [ 0.2300086 ],\n",
       "       [-0.21572648],\n",
       "       [ 0.45203081],\n",
       "       [ 0.23398159],\n",
       "       [ 0.13809773],\n",
       "       [ 0.27090771],\n",
       "       [ 0.81683241],\n",
       "       [ 0....\n",
       "       [ 0.07277798],\n",
       "       [ 0.40024767],\n",
       "       [ 0.25483221],\n",
       "       [ 0.29498287],\n",
       "       [ 0.23015708],\n",
       "       [ 0.35364444],\n",
       "       [ 0.4470003 ],\n",
       "       [ 0.19220651],\n",
       "       [ 0.33440007],\n",
       "       [ 0.76201343],\n",
       "       [ 0.73911218],\n",
       "       [ 0.32547584],\n",
       "       [ 0.37819943],\n",
       "       [ 0.10667913],\n",
       "       [ 0.61625224],\n",
       "       [ 0.29957089],\n",
       "       [ 0.52686572],\n",
       "       [ 0.73844867],\n",
       "       [ 0.24180062],\n",
       "       [ 0.97689652],\n",
       "       [ 0.52715094],\n",
       "       [ 0.49581094],\n",
       "       [ 0.40625472],\n",
       "       [ 0.67021155],\n",
       "       [ 0.24211985],\n",
       "       [ 0.05364685],\n",
       "       [ 0.54554987]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_2.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR-AUC: 0.6816232994386251\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_2.predict(X_test_encoded)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(\"PR-AUC:\", pr_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Время, необходимое для обучения модели увеличилось почти в два раза.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно было заменить, one-hot-кодирование сильно увилечивает количество признаков в датасете, что сказывается на памяти, особенно, если некоторый признак имеет большое количество значений. Эту проблему решает другой способ кодирование категориальных признаков — счётчики. Основная идея в том, что нам важны не сами категории, а значения целевой переменной, которые имеют объекты этой категории. Каждый категориальный признак мы заменим средним значением целевой переменной по всем объектам этой же категории:\n",
    "$$\n",
    "g_j(x, X) = \\frac{\\sum_{i=1}^{l} [f_j(x) = f_j(x_i)][y_i = +1]}{\\sum_{i=1}^{l} [f_j(x) = f_j(x_i)]}\n",
    "$$\n",
    "\n",
    "__Задание 7 (1.5 балла).__ Закодируйте категориальные переменные с помощью счётчиков (ровно так, как описано выше, без каких-либо хитростей). Обучите логистическую регрессию и посмотрите на качество модели на тестовом множестве. Сравните время обучения с предыдущим экспериментом. Заметили ли вы что-то интересное?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categorical_columns:\n",
    "    full = pd.concat([X_train.reset_index(drop=True), pd.Series(y_train, name='target')], axis=1)\n",
    "    replace = full.groupby(category).agg({'target': 'mean'}).to_dict()['target']\n",
    "    X_train[category] = X_train[category].replace(replace)\n",
    "    X_test[category] = X_test[category].replace(replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1298403/4110918471.py:101: RuntimeWarning: overflow encountered in exp\n",
      "  probas = 1 / (1 + np.exp(-args))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.77 s, sys: 4.59 s, total: 6.36 s\n",
      "Wall time: 1.12 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogReg(batch_size=1000,\n",
       "       w0=array([[-1.33028050e+01],\n",
       "       [-9.82458746e+00],\n",
       "       [-3.35267225e+01],\n",
       "       [-2.11187545e+00],\n",
       "       [-1.84951997e+00],\n",
       "       [-2.52079009e+00],\n",
       "       [-1.33987369e+00],\n",
       "       [-5.05259464e-01],\n",
       "       [-1.19227209e+00],\n",
       "       [ 6.19635308e-01],\n",
       "       [ 7.01396027e-01],\n",
       "       [-9.59019251e-03],\n",
       "       [ 7.24126352e-01],\n",
       "       [ 5.69402547e-01],\n",
       "       [-5.75731215e+01],\n",
       "       [-5.30834201e+00],\n",
       "       [-2.64187613e-01],\n",
       "       [-6.58934573e-0...\n",
       "       [-2.53542549e+00],\n",
       "       [-2.67591721e+00],\n",
       "       [-3.04098603e+00],\n",
       "       [-1.78764539e+01],\n",
       "       [-1.39554558e+01],\n",
       "       [-5.73970976e+01],\n",
       "       [-2.15775884e+01],\n",
       "       [-7.00177379e+01],\n",
       "       [-1.74690454e+01],\n",
       "       [-6.33145591e+01],\n",
       "       [-4.01334315e+01],\n",
       "       [-1.10061374e+01],\n",
       "       [-2.12134799e+01],\n",
       "       [-5.66231569e+01],\n",
       "       [-6.31585965e-01],\n",
       "       [-3.81564079e+00],\n",
       "       [-3.38641007e+00],\n",
       "       [-1.49727227e+00],\n",
       "       [-2.12838979e+00],\n",
       "       [-3.54026771e-01]]))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogReg</label><div class=\"sk-toggleable__content\"><pre>LogReg(batch_size=1000,\n",
       "       w0=array([[-1.33028050e+01],\n",
       "       [-9.82458746e+00],\n",
       "       [-3.35267225e+01],\n",
       "       [-2.11187545e+00],\n",
       "       [-1.84951997e+00],\n",
       "       [-2.52079009e+00],\n",
       "       [-1.33987369e+00],\n",
       "       [-5.05259464e-01],\n",
       "       [-1.19227209e+00],\n",
       "       [ 6.19635308e-01],\n",
       "       [ 7.01396027e-01],\n",
       "       [-9.59019251e-03],\n",
       "       [ 7.24126352e-01],\n",
       "       [ 5.69402547e-01],\n",
       "       [-5.75731215e+01],\n",
       "       [-5.30834201e+00],\n",
       "       [-2.64187613e-01],\n",
       "       [-6.58934573e-0...\n",
       "       [-2.53542549e+00],\n",
       "       [-2.67591721e+00],\n",
       "       [-3.04098603e+00],\n",
       "       [-1.78764539e+01],\n",
       "       [-1.39554558e+01],\n",
       "       [-5.73970976e+01],\n",
       "       [-2.15775884e+01],\n",
       "       [-7.00177379e+01],\n",
       "       [-1.74690454e+01],\n",
       "       [-6.33145591e+01],\n",
       "       [-4.01334315e+01],\n",
       "       [-1.10061374e+01],\n",
       "       [-2.12134799e+01],\n",
       "       [-5.66231569e+01],\n",
       "       [-6.31585965e-01],\n",
       "       [-3.81564079e+00],\n",
       "       [-3.38641007e+00],\n",
       "       [-1.49727227e+00],\n",
       "       [-2.12838979e+00],\n",
       "       [-3.54026771e-01]]))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogReg(batch_size=1000,\n",
       "       w0=array([[-1.33028050e+01],\n",
       "       [-9.82458746e+00],\n",
       "       [-3.35267225e+01],\n",
       "       [-2.11187545e+00],\n",
       "       [-1.84951997e+00],\n",
       "       [-2.52079009e+00],\n",
       "       [-1.33987369e+00],\n",
       "       [-5.05259464e-01],\n",
       "       [-1.19227209e+00],\n",
       "       [ 6.19635308e-01],\n",
       "       [ 7.01396027e-01],\n",
       "       [-9.59019251e-03],\n",
       "       [ 7.24126352e-01],\n",
       "       [ 5.69402547e-01],\n",
       "       [-5.75731215e+01],\n",
       "       [-5.30834201e+00],\n",
       "       [-2.64187613e-01],\n",
       "       [-6.58934573e-0...\n",
       "       [-2.53542549e+00],\n",
       "       [-2.67591721e+00],\n",
       "       [-3.04098603e+00],\n",
       "       [-1.78764539e+01],\n",
       "       [-1.39554558e+01],\n",
       "       [-5.73970976e+01],\n",
       "       [-2.15775884e+01],\n",
       "       [-7.00177379e+01],\n",
       "       [-1.74690454e+01],\n",
       "       [-6.33145591e+01],\n",
       "       [-4.01334315e+01],\n",
       "       [-1.10061374e+01],\n",
       "       [-2.12134799e+01],\n",
       "       [-5.66231569e+01],\n",
       "       [-6.31585965e-01],\n",
       "       [-3.81564079e+00],\n",
       "       [-3.38641007e+00],\n",
       "       [-1.49727227e+00],\n",
       "       [-2.12838979e+00],\n",
       "       [-3.54026771e-01]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_3 = LogReg(batch_size=1000)\n",
    "model_3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR-AUC: 0.518148904177482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1298403/4110918471.py:101: RuntimeWarning: overflow encountered in exp\n",
      "  probas = 1 / (1 + np.exp(-args))\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_3.predict(X_test)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(\"PR-AUC:\", pr_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Качество упало, но скорость сильно возросла**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отметим, что такие признаки сами по себе являются классификаторами и, обучаясь на них, мы допускаем \"утечку\" целевой переменной в признаки. Это ведёт к переобучению, поэтому считать такие признаки необходимо так, чтобы при вычислении для конкретного объекта его целевая метка не использовалась. Это можно делать следующими способами:\n",
    "1. Вычислять значение счётчика по всем объектам расположенным выше в датасете (например, если у нас выборка отсортирована по времени).\n",
    "2. Вычислять по фолдам, то есть делить выборку на некоторое количество частей и подсчитывать значение признаков по всем фолдам кроме текущего (как делается в кросс-валидации).\n",
    "3. Внесение некоторого шума в посчитанные признаки. \n",
    "\n",
    "__Задание 8 (1 балл)__. Реализуйте корректное вычисление счётчиков самым простым способом — добавление шума к значениям (необходимо соблюсти баланс между избавление от переобучения и полезностью признаков). Снова обучите логистическую регрессию, оцените качество. Сделайте выводы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Отбор признаков (3 балла + 1 бонус)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важной частью процесса построения модели является отбор признаков. На практике многие признаки оказывают малое влияние на модель (при этом они увеличивают время вычислений) или даже негативно сказываются на качестве модели. Попробуем несколько подходов отбора признаков, оценим, как они влияют на качество модели и сколько времени занимают.\n",
    "\n",
    "Обратимся к тому же датасету про обращение клиентов по страховым случаям. Обойдёмся без сэмплирования объектов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T07:36:04.765536Z",
     "start_time": "2018-10-12T07:35:57.814973Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv', index_col=0)\n",
    "target = data.target.values\n",
    "data = data.drop('target', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=124)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы помните, в данных много категориальных признаков. Давайте закодируем их с помощью one-hot кодирования. Исходные колонки с категориальными признаками можно удалить. Сколько признаков мы получили?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [column for column in X_train.columns if \"cat\" in column]\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "X_train_encoded = encoder.fit_transform(X_train[categorical_columns])\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out())\n",
    "X_train_encoded = pd.concat([X_train.drop(columns=categorical_columns), X_train_encoded], axis=1, join='inner')\n",
    "\n",
    "X_test_encoded = encoder.transform(X_test[categorical_columns])\n",
    "X_test_encoded = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out())\n",
    "X_test_encoded = pd.concat([X_test.drop(columns=categorical_columns), X_test_encoded], axis=1, join='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of         ps_ind_01  ps_ind_03  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  \\\n",
       "0               1          8              0              0              1   \n",
       "1               2          2              0              0              0   \n",
       "2               0          2              1              0              0   \n",
       "3               6         11              0              1              0   \n",
       "4               3          5              0              0              1   \n",
       "...           ...        ...            ...            ...            ...   \n",
       "476164          2          6              1              0              0   \n",
       "476165          1          7              1              0              0   \n",
       "476166          3          4              0              0              0   \n",
       "476167          3          1              0              1              0   \n",
       "476168          1          0              0              0              1   \n",
       "\n",
       "        ps_ind_09_bin  ps_ind_10_bin  ps_ind_11_bin  ps_ind_12_bin  \\\n",
       "0                   0              0              0              0   \n",
       "1                   1              0              0              0   \n",
       "2                   0              0              0              0   \n",
       "3                   0              0              0              0   \n",
       "4                   0              0              0              1   \n",
       "...               ...            ...            ...            ...   \n",
       "476164              0              0              0              0   \n",
       "476165              0              0              0              0   \n",
       "476166              1              0              0              0   \n",
       "476167              0              0              0              0   \n",
       "476168              0              0              0              0   \n",
       "\n",
       "        ps_ind_13_bin  ...  ps_car_11_cat_95  ps_car_11_cat_96  \\\n",
       "0                   0  ...             False             False   \n",
       "1                   0  ...             False             False   \n",
       "2                   0  ...             False             False   \n",
       "3                   0  ...             False             False   \n",
       "4                   0  ...             False             False   \n",
       "...               ...  ...               ...               ...   \n",
       "476164              0  ...             False             False   \n",
       "476165              0  ...             False             False   \n",
       "476166              0  ...             False             False   \n",
       "476167              0  ...             False             False   \n",
       "476168              0  ...             False             False   \n",
       "\n",
       "        ps_car_11_cat_97  ps_car_11_cat_98  ps_car_11_cat_99  \\\n",
       "0                  False             False             False   \n",
       "1                  False             False             False   \n",
       "2                  False             False             False   \n",
       "3                  False             False             False   \n",
       "4                  False             False             False   \n",
       "...                  ...               ...               ...   \n",
       "476164             False             False             False   \n",
       "476165             False             False             False   \n",
       "476166             False             False             False   \n",
       "476167             False             False             False   \n",
       "476168             False             False             False   \n",
       "\n",
       "        ps_car_11_cat_100  ps_car_11_cat_101  ps_car_11_cat_102  \\\n",
       "0                   False              False              False   \n",
       "1                   False              False              False   \n",
       "2                   False              False              False   \n",
       "3                   False              False              False   \n",
       "4                   False              False              False   \n",
       "...                   ...                ...                ...   \n",
       "476164              False              False              False   \n",
       "476165              False              False              False   \n",
       "476166              False              False              False   \n",
       "476167              False              False              False   \n",
       "476168              False              False              False   \n",
       "\n",
       "        ps_car_11_cat_103  ps_car_11_cat_104  \n",
       "0                   False              False  \n",
       "1                   False              False  \n",
       "2                   False              False  \n",
       "3                   False              False  \n",
       "4                   False              False  \n",
       "...                   ...                ...  \n",
       "476164              False               True  \n",
       "476165              False               True  \n",
       "476166              False              False  \n",
       "476167              False              False  \n",
       "476168              False              False  \n",
       "\n",
       "[476169 rows x 227 columns]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве основной модели будем использовать логистическую регрессию, а целевой метрики — ROC-AUC. Обучите модель и посчитайте качество на тестовой выборке. Давайте запомним полученное значение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1298403/4110918471.py:101: RuntimeWarning: overflow encountered in exp\n",
      "  probas = 1 / (1 + np.exp(-args))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.29 s, sys: 10.3 s, total: 14.5 s\n",
      "Wall time: 3.81 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogReg(batch_size=1000,\n",
       "       w0=array([[-1.39985742e+01],\n",
       "       [-3.44228749e+01],\n",
       "       [-3.08430625e+00],\n",
       "       [-1.39490293e+00],\n",
       "       [-1.04787498e+00],\n",
       "       [-1.13505183e+00],\n",
       "       [ 4.68950691e-01],\n",
       "       [ 3.84325981e-01],\n",
       "       [ 1.50949607e-01],\n",
       "       [ 6.57654731e-01],\n",
       "       [ 2.49542893e-01],\n",
       "       [-5.68711141e+01],\n",
       "       [-4.91771171e+00],\n",
       "       [-5.58289033e-01],\n",
       "       [-1.09289989e+00],\n",
       "       [-4.73350871e+00],\n",
       "       [-2.64259859e+00],\n",
       "       [-3.47024099e+00...\n",
       "       [ 1.67008671e-01],\n",
       "       [ 8.77987250e-02],\n",
       "       [ 1.96476108e-01],\n",
       "       [ 7.64579308e-01],\n",
       "       [ 6.39725858e-01],\n",
       "       [-4.60137978e-02],\n",
       "       [ 8.65585081e-01],\n",
       "       [ 8.14116387e-01],\n",
       "       [ 2.73050466e-01],\n",
       "       [ 7.31157533e-01],\n",
       "       [ 3.63117037e-01],\n",
       "       [ 1.38961290e-01],\n",
       "       [ 9.20335805e-01],\n",
       "       [ 1.14942093e-01],\n",
       "       [ 3.75531255e-01],\n",
       "       [ 5.78631713e-01],\n",
       "       [ 6.37390772e-01],\n",
       "       [ 4.49217937e-01],\n",
       "       [ 8.49500946e-01],\n",
       "       [-1.54516974e-02],\n",
       "       [-2.98430403e-01]]))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogReg</label><div class=\"sk-toggleable__content\"><pre>LogReg(batch_size=1000,\n",
       "       w0=array([[-1.39985742e+01],\n",
       "       [-3.44228749e+01],\n",
       "       [-3.08430625e+00],\n",
       "       [-1.39490293e+00],\n",
       "       [-1.04787498e+00],\n",
       "       [-1.13505183e+00],\n",
       "       [ 4.68950691e-01],\n",
       "       [ 3.84325981e-01],\n",
       "       [ 1.50949607e-01],\n",
       "       [ 6.57654731e-01],\n",
       "       [ 2.49542893e-01],\n",
       "       [-5.68711141e+01],\n",
       "       [-4.91771171e+00],\n",
       "       [-5.58289033e-01],\n",
       "       [-1.09289989e+00],\n",
       "       [-4.73350871e+00],\n",
       "       [-2.64259859e+00],\n",
       "       [-3.47024099e+00...\n",
       "       [ 1.67008671e-01],\n",
       "       [ 8.77987250e-02],\n",
       "       [ 1.96476108e-01],\n",
       "       [ 7.64579308e-01],\n",
       "       [ 6.39725858e-01],\n",
       "       [-4.60137978e-02],\n",
       "       [ 8.65585081e-01],\n",
       "       [ 8.14116387e-01],\n",
       "       [ 2.73050466e-01],\n",
       "       [ 7.31157533e-01],\n",
       "       [ 3.63117037e-01],\n",
       "       [ 1.38961290e-01],\n",
       "       [ 9.20335805e-01],\n",
       "       [ 1.14942093e-01],\n",
       "       [ 3.75531255e-01],\n",
       "       [ 5.78631713e-01],\n",
       "       [ 6.37390772e-01],\n",
       "       [ 4.49217937e-01],\n",
       "       [ 8.49500946e-01],\n",
       "       [-1.54516974e-02],\n",
       "       [-2.98430403e-01]]))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogReg(batch_size=1000,\n",
       "       w0=array([[-1.39985742e+01],\n",
       "       [-3.44228749e+01],\n",
       "       [-3.08430625e+00],\n",
       "       [-1.39490293e+00],\n",
       "       [-1.04787498e+00],\n",
       "       [-1.13505183e+00],\n",
       "       [ 4.68950691e-01],\n",
       "       [ 3.84325981e-01],\n",
       "       [ 1.50949607e-01],\n",
       "       [ 6.57654731e-01],\n",
       "       [ 2.49542893e-01],\n",
       "       [-5.68711141e+01],\n",
       "       [-4.91771171e+00],\n",
       "       [-5.58289033e-01],\n",
       "       [-1.09289989e+00],\n",
       "       [-4.73350871e+00],\n",
       "       [-2.64259859e+00],\n",
       "       [-3.47024099e+00...\n",
       "       [ 1.67008671e-01],\n",
       "       [ 8.77987250e-02],\n",
       "       [ 1.96476108e-01],\n",
       "       [ 7.64579308e-01],\n",
       "       [ 6.39725858e-01],\n",
       "       [-4.60137978e-02],\n",
       "       [ 8.65585081e-01],\n",
       "       [ 8.14116387e-01],\n",
       "       [ 2.73050466e-01],\n",
       "       [ 7.31157533e-01],\n",
       "       [ 3.63117037e-01],\n",
       "       [ 1.38961290e-01],\n",
       "       [ 9.20335805e-01],\n",
       "       [ 1.14942093e-01],\n",
       "       [ 3.75531255e-01],\n",
       "       [ 5.78631713e-01],\n",
       "       [ 6.37390772e-01],\n",
       "       [ 4.49217937e-01],\n",
       "       [ 8.49500946e-01],\n",
       "       [-1.54516974e-02],\n",
       "       [-2.98430403e-01]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_4 = LogReg(batch_size=1000)\n",
    "model_4.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR-AUC: 0.518148904177482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1298403/4110918471.py:101: RuntimeWarning: overflow encountered in exp\n",
      "  probas = 1 / (1 + np.exp(-args))\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_4.predict(X_test_encoded)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(\"PR-AUC:\", pr_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Встроенные методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, мы хотим оставить только 200 лучших признаков. Попробуем сделать это несколькими способами.\n",
    "\n",
    "Начнём с отборам признаков с помощью линейной модели. Как известно, веса линейной модели означают вклад каждого признака в предсказание модели, а значит, модуль этого вклада можно интерпретировать как важность признаков. Такой метод отбора называются встроенным или embedded methods, так как он заложен в особенности модели.\n",
    "\n",
    "__Задание 10 (1 балл).__ Оставьте 200 признаков с наибольшим модулем соответсвующего параметра линейной модели. Обучите модели заново и оцените её качество. Замерьте скорость такого отбора признаков.\n",
    "\n",
    "Изменилось ли качество? Как?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь давайте подумаем, что мы не учли. Мы предположили, что признаки вносят вклад равномерно, но не учли их масштаба. Если мы умножим один из признаков в 100 раз, то без учёта регуляризации его вес уменьшится в эти же 100 раз. А мы на основе этого отбираем признаки! Давайте сначала отнормируем признаки одним из способов, а только потом будем удалять признаки. \n",
    "\n",
    "Кстати, в таком случае надо пересчитать качество на всех признаках (сделайте это ниже). Если вы сделали нормирование признаков в самом начале, то попробуйте отобрать признаки на неотмасштабированных данных.\n",
    "\n",
    "Что получилось?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 10 (0.5 балла).__\n",
    "Можно задать отбор признаков более функционально. Вспомним, что L1-регуляризация также умеет отбирать признаки. Понятно, что теперь нам будет сложнее оставить именно 200 лучших признаков, но возможно они нам и не нужны. Подберите коэффициент регуляризации и проверьте, как изменилось качество. Получилось ли добиться лучшего качества при менее чем 200 признаках?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Методы фильтрации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте отбирать признаки умнее, а именно через подсчёт некоторой функции для каждого признака. На основании значений этой функции будем оставлять наиболее важные признаки. Методы этого семейства называют фильтрующими или  filter methods. \n",
    "\n",
    "В качестве такой функции будем считать t-статистику:\n",
    "\n",
    "$$t(x) = \\frac{|\\mu_+ - \\mu_-|}{\\sqrt{\\frac{n_+ \\sigma^2_+ + n_- \\sigma^2_-}{n_+ + n_-}}},$$\n",
    "\n",
    "где $mu$, $sigma$, $n$ соответственно среднее, среднеквадратичное отклонение и количество объектов каждого из классов.\n",
    "\n",
    "__Задание 11 (1 балл)__. Оставьте 200 признаков с наибольшим значением и замерьте качество. Не забудьте замерить скорость отбора признаков в этом случаев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Методы-обёртки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 12 (бонус, 1 балл)__.\n",
    "\n",
    "\n",
    "Заключительный из рассматриваемых нами методов работает следующим образом: мы исключаем по очереди один из признаков и смотрим, как это влияет на качество. Удаляем признаки таким жадным способом, пока не достигнем некоторого критерия (количество признаков или ухудшением качества).\n",
    "\n",
    "Заметим, что нельзя оценивать качество по тестовой выборке, иначе мы можем переобучиться, как, например, при настройке гиперпараметров. Разделите выборку на 2 части, на одной из них обучайте модель без одного из признаков,  на второй части оценивайте качество. Исходную тестовую выборку стоит использовать только на финальной оценке качества.\n",
    "\n",
    "Сделайте одну итерацию и прикиньте, сколько времени займёт такой отбор признаков. Кажется, что чересчур. Давайте возьмём маленький сэмпл данных (например, в 10 тысяч объектов), что сильно уменьшит время итерации. Теперь это долго, но уже приемлимо. \n",
    "\n",
    "Если это всё ещё долго для вашего комьютера, можете попробовать брать не по одному признаку, а сразу по пять (и удалять сразу тоже по 5). Для этого перед каждой итерацией удаления делите заново все признаки на группы по 5 штук.\n",
    "\n",
    "Снова оставьте только 200 признаков и оцените качество на тестовой выборке. Сколько времени занял такой отбор признаков?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опционально (это не оценивается) можете рассмотреть более интересные стратегии отбора, чем жадная. Например, генетические алгоритмы. Можно закодировать бинарным вектором, включаем мы или нет тот или иной признак в модель. А дальше генетическим алгоритмом оптимизировать этот вектор. Всё ещё не быстро, но точно быстрее жадного.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 13 (0.5 балла)** Давайте подведём итоги по отбору признаков. Назовите преимущества и недостатки каждого из методов. Какой метод привёл к наилучшему качеству? Если не делали бонус — сравните встроенный метод и метод фильтрации.\n",
    "\n",
    "**Ответ:** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
